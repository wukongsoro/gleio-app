# Lovable Prompt Integration - Anti-Hallucination Rules

## ğŸ¯ Goal: Simplicity + No Hallucinations

After analyzing Lovable's prompt, I've integrated the **most valuable anti-hallucination principles** that keep the AI focused, simple, and reliable.

---

## âœ… What Was Added

### **1. Scope Control (Do Exactly What's Asked)**
```
âœ… Implement ONLY what the user explicitly requested
âœ… If request is vague, ask ONE clarifying question
âœ… Check if feature already exists before modifying
âŒ NO "nice-to-have" features
âŒ NO scope creep
âŒ NO overengineering
```

**Why:** Prevents AI from adding unnecessary features, staying laser-focused on user's actual request.

---

### **2. Discussion Mode (Default for Unclear Requests)**
```
âœ… Assume user wants to discuss/plan if request is ambiguous
âœ… Only implement when user uses action words: "create", "build", "implement"
âœ… For questions, provide explanation without code changes
âŒ NO jumping straight to implementation for informational questions
âŒ NO guessing what user wants
```

**Why:** Prevents AI from building the wrong thing when user is just asking a question.

---

### **3. Architectural Simplicity**
```
âœ… Create small, focused components (not monolithic 1000-line files)
âœ… Prefer search-replace for modifications over rewrites
âœ… Refactor only if code is genuinely messy
âœ… Make small, verifiable changes
âŒ NO large files
âŒ NO doing too much at once
```

**Why:** Keeps code maintainable and prevents overwhelming changes.

---

### **4. Common Pitfalls to Avoid**
```
âŒ NO VITE_* env variables (not supported in WebContainer)
âŒ NO reading files already in context
âŒ NO sequential tool calls that could be batched
âŒ NO experimental/unstable libraries
âŒ NO hardcoded values
âŒ NO custom inline styles
âŒ NO inventing features
```

**Why:** Explicitly warns AI about common mistakes that cause failures.

---

## ğŸ“Š Key Lovable Principles Integrated

| Lovable Principle | How It Prevents Hallucination |
|-------------------|-------------------------------|
| **"Check Understanding"** | AI asks clarifying questions instead of guessing |
| **"Be Concise"** | Reduces token usage and over-explanation |
| **"Scope Control"** | Prevents adding features user didn't ask for |
| **"Discussion First"** | Avoids premature implementation |
| **"Perfect Architecture"** | Keeps code clean and focused |
| **"Avoid Overengineering"** | Prioritizes simple, working solutions |

---

## ğŸš« What We Did NOT Add (Too Complex)

We intentionally **excluded** these Lovable-specific features to avoid overcomplication:

1. âŒ **Design system enforcement** (too opinionated for our use case)
2. âŒ **Shadcn-specific rules** (we support multiple frameworks)
3. âŒ **First message special handling** (adds unnecessary complexity)
4. âŒ **Mermaid diagram requirements** (not core to functionality)
5. âŒ **SEO requirements on every page** (too prescriptive)

**Reason:** These rules are specific to Lovable's Vite+React+Shadcn workflow. Gleio AI supports Next.js, Vite, multiple UI libraries, and full-stack apps, so we kept it framework-agnostic.

---

## ğŸ¯ Result: Simple + Effective

### **Before Integration:**
- âŒ AI sometimes added features user didn't ask for
- âŒ AI sometimes guessed instead of asking clarifying questions
- âŒ AI sometimes created large, monolithic files

### **After Integration:**
- âœ… AI stays focused on explicit user request
- âœ… AI asks for clarification when request is vague
- âœ… AI creates small, maintainable components
- âœ… AI avoids common pitfalls (env vars, large files, etc.)

---

## ğŸ“ Files Modified

- `app/lib/.server/llm/prompts.ts` (lines 200-234)
  - Added `<anti_hallucination_rules>` section after `<communication_style>`

---

## ğŸš€ Impact

| Before | After |
|--------|-------|
| Sometimes adds extra features | Only implements what's asked |
| Guesses when unclear | Asks ONE clarifying question |
| Creates large files | Small, focused components |
| Occasional scope creep | Laser-focused on request |

---

## âœ¨ Summary

We've integrated **Lovable's best anti-hallucination principles** without adding unnecessary complexity:

1. âœ… **Scope control** - Do exactly what's asked, nothing more
2. âœ… **Discussion mode** - Ask before assuming
3. âœ… **Architectural simplicity** - Small components, clean code
4. âœ… **Common pitfalls** - Explicit warnings about known issues

**The AI will now be more focused, reliable, and less likely to hallucinate features or versions!** ğŸ‰
